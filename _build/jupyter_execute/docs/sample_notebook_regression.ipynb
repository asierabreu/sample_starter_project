{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d92f4535",
   "metadata": {},
   "source": [
    "# Sample Notebook ( Regression )\n",
    "\n",
    "This is a sample notebook for a regression type of ML application"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2599fef",
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import rcParams, cycler\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ef49df9",
   "metadata": {},
   "source": [
    "### Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58b3ab45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load and summarize the california housing dataset\n",
    "from sklearn.datasets import fetch_california_housing\n",
    "dataset = fetch_california_housing(return_X_y=False, as_frame=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad20e728",
   "metadata": {},
   "outputs": [],
   "source": [
    "# checkout the structure and shape of the dataset\n",
    "dataset.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e326ba05",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aff75054",
   "metadata": {},
   "source": [
    "### Data Inspection\n",
    "Look at :\n",
    " 1. Data distributions \n",
    " 2. Basic statistics\n",
    " 3. Correlations between features and target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39937daf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# inspect dataset\n",
    "print('dataset instances : %d' %len(dataset.data))\n",
    "print('dataset features  : %s' %len(dataset.feature_names))\n",
    "print('dataset atributes : %s' %dataset.feature_names)\n",
    "print('dataset feature   : %s' %dataset.target_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "666c7204",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = dataset.data\n",
    "y = dataset.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d553785",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Distributions\n",
    "X.hist(bins=80, figsize=(15, 15), grid=False);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de7e1506",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic statistics\n",
    "X.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b77b1f8d",
   "metadata": {},
   "source": [
    "From the histograms and basic statistics we can already see here 2 important points regarding the dataset:\n",
    "\n",
    " 1. Basic Statistics : some of the attributes contain outliers, like AveRooms and AveBedrms  \n",
    " 2. Distributions : the scales of the attributes are quite different\n",
    "\n",
    "\n",
    " Conclusions:\n",
    "\n",
    " 1. Some outliers treatment is necessary (removal of outliers for example)\n",
    " 2. Some data standarization is also necessary to bring all features into an equivalent scale . This is done to avoid the variance scale from a large feature to dominate and bias the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a24af32a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correlations between feature and target\n",
    "import seaborn as sns\n",
    "corr = dataset.data.corr()\n",
    "corr.style.background_gradient(cmap='coolwarm').set_precision(2)\n",
    "\n",
    "# Generate a mask for the upper triangle\n",
    "mask = np.triu(np.ones_like(corr, dtype=bool))\n",
    "\n",
    "# Set up the matplotlib figure\n",
    "f, ax = plt.subplots(figsize=(11, 9))\n",
    "\n",
    "# Generate a custom diverging colormap\n",
    "cmap = sns.diverging_palette(230, 20, as_cmap=True)\n",
    "\n",
    "# Draw the heatmap with the mask and correct aspect ratio\n",
    "sns.heatmap(corr, mask=mask, cmap=cmap, vmax=.3, center=0,\n",
    "            square=True, linewidths=.5);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01d0743c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# outlier detection using covariance\n",
    "from sklearn.covariance import EllipticEnvelope\n",
    "cov = EllipticEnvelope(random_state=0).fit(X)\n",
    "# check outliers on each attribute : predict returns 1 for an inlier and -1 for an outlier\n",
    "covariances=cov.predict(X)\n",
    "outliers=[i for i in range(len(covariances)) if covariances[i] == -1]\n",
    "print('found : %2d outliers in data' %len(outliers))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f75a6b9a",
   "metadata": {},
   "source": [
    "We can already see there is a stronger correlation between Average Rooms for example and Mean House Value.\n",
    "We could use this in order to select stronger features for training."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94615ef6",
   "metadata": {},
   "source": [
    "### Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "959b3b21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply transformations to the attrributes and target\n",
    "from sklearn.preprocessing import *\n",
    "X_scaled = MinMaxScaler().fit_transform(X) \n",
    "y_scaled = MinMaxScaler().fit_transform(y.values.reshape(-1, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "912630eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate several regression algorithms on the dataset and create several models\n",
    "from sklearn.linear_model import ElasticNet\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.feature_selection import SequentialFeatureSelector\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import RepeatedKFold\n",
    "\n",
    "# define a dictionary with the algorithms we would like to test out on the data\n",
    "# Note : model hyperparameters here are not tuned!\n",
    "models = {\n",
    "    'LinearRegression' : LinearRegression(),\n",
    "    'ElasticNet' : ElasticNet(alpha=1.0, l1_ratio=0.5),\n",
    "    'RandomForestRegressor' : RandomForestRegressor(n_estimators=10),\n",
    "}\n",
    "\n",
    "# define model evaluation method\n",
    "cv = RepeatedKFold(n_splits=10, n_repeats=3, random_state=1)\n",
    "for name,model in models.items():\n",
    "    # evaluate model\n",
    "    scores = cross_val_score(model, X_scaled, y_scaled, scoring='neg_mean_absolute_error', cv=cv, n_jobs=-1)\n",
    "    # force scores to be positive\n",
    "    scores = abs(scores)\n",
    "    print('Model : %-24s Mean MAE: %.3f (%.3f)' % (name , scores.mean(), scores.std()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "543efd8a",
   "metadata": {},
   "source": [
    "### Model Predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc3df3db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select the best model and make a prediction with it\n",
    "selected_model='RandomForestRegressor'\n",
    "model = models[selected_model]\n",
    "# fit model\n",
    "model.fit(X_scaled, y_scaled)\n",
    "# define new data to predict the value of the house\n",
    "new_housing_data = [8.32,41,6.98,1,322,2.55,37.88,-122]\n",
    "scaler=MinMaxScaler().fit(np.array(new_housing_data).reshape(-1,1))\n",
    "new_housing_data_scaled = scaler.transform(np.array(new_housing_data).reshape(-1,1))\n",
    "# make a prediction\n",
    "y_predicted = model.predict(new_housing_data_scaled.T)\n",
    "y_scaler = MinMaxScaler().fit(y.values.reshape(-1,1))\n",
    "# summarize prediction , inverse back the scaled prediction\n",
    "print('House Predicted Value (KDollar): %.3f' %y_scaler.inverse_transform(y_predicted.reshape(-1,1)))"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "text_representation": {
    "extension": ".md",
    "format_name": "myst",
    "format_version": 0.13,
    "jupytext_version": "1.14.5"
   }
  },
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "venv"
  },
  "source_map": [
   12,
   18,
   25,
   29,
   35,
   40,
   42,
   50,
   58,
   63,
   68,
   71,
   84,
   104,
   112,
   117,
   121,
   128,
   156,
   160
  ]
 },
 "nbformat": 4,
 "nbformat_minor": 5
}